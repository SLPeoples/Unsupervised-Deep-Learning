{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the appropriate modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from util import relu, error_rate, getKaggleMNIST, init_weights\n",
    "from autoencoder import DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    def __init__(self, M, an_id):\n",
    "        self.M = M\n",
    "        self.id = an_id\n",
    "        self.rng = RandomStreams()\n",
    "\n",
    "    def fit(self, X, learning_rate=0.1, epochs=1, batch_sz=100, show_fig=False):\n",
    "        N, D = X.shape\n",
    "        n_batches = N / batch_sz\n",
    "\n",
    "        W0 = init_weights((D, self.M))\n",
    "        self.W = theano.shared(W0, 'W_%s' % self.id)\n",
    "        self.c = theano.shared(np.zeros(self.M), 'c_%s' % self.id)\n",
    "        self.b = theano.shared(np.zeros(D), 'b_%s' % self.id)\n",
    "        self.params = [self.W, self.c, self.b]\n",
    "        self.forward_params = [self.W, self.c]\n",
    "\n",
    "        # we won't use this to fit the RBM but we will use these for backpropagation later\n",
    "        # TODO: technically they should be reset before doing backprop\n",
    "        self.dW = theano.shared(np.zeros(W0.shape), 'dW_%s' % self.id)\n",
    "        self.dc = theano.shared(np.zeros(self.M), 'dbh_%s' % self.id)\n",
    "        self.db = theano.shared(np.zeros(D), 'dbo_%s' % self.id)\n",
    "        self.dparams = [self.dW, self.dc, self.db]\n",
    "        self.forward_dparams = [self.dW, self.dc]\n",
    "\n",
    "        X_in = T.matrix('X_%s' % self.id)\n",
    "\n",
    "        # attach it to the object so it can be used later\n",
    "        # must be sigmoidal because the output is also a sigmoid\n",
    "        H = T.nnet.sigmoid(X_in.dot(self.W) + self.c)\n",
    "        self.hidden_op = theano.function(\n",
    "            inputs=[X_in],\n",
    "            outputs=H,\n",
    "        )\n",
    "\n",
    "        # we won't use this cost to do any updates\n",
    "        # but we would like to see how this cost function changes\n",
    "        # as we do contrastive divergence\n",
    "        X_hat = self.forward_output(X_in)\n",
    "        cost = -(X_in * T.log(X_hat) + (1 - X_in) * T.log(1 - X_hat)).sum() / (batch_sz * D)\n",
    "        cost_op = theano.function(\n",
    "            inputs=[X_in],\n",
    "            outputs=cost,\n",
    "        )\n",
    "\n",
    "        # do one round of Gibbs sampling to obtain X_sample\n",
    "        H = self.sample_h_given_v(X_in)\n",
    "        X_sample = self.sample_v_given_h(H)\n",
    "\n",
    "        # define the objective, updates, and train function\n",
    "        objective = T.mean(self.free_energy(X_in)) - T.mean(self.free_energy(X_sample))\n",
    "\n",
    "        # need to consider X_sample constant because you can't take the gradient of random numbers in Theano\n",
    "        updates = [(p, p - learning_rate*T.grad(objective, p, consider_constant=[X_sample])) for p in self.params]\n",
    "        train_op = theano.function(\n",
    "            inputs=[X_in],\n",
    "            updates=updates,\n",
    "        )\n",
    "\n",
    "        costs = []\n",
    "        print (\"training rbm: %s\" % self.id)\n",
    "        for i in range(int(epochs)):\n",
    "            print (\"epoch:\", i)\n",
    "            X = shuffle(X)\n",
    "            for j in range(int(n_batches)):\n",
    "                batch = X[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                train_op(batch)\n",
    "                the_cost = cost_op(X)  # technically we could also get the cost for Xtest here\n",
    "                print (\"j / n_batches:\", j, \"/\", n_batches, \"cost:\", the_cost)\n",
    "                costs.append(the_cost)\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "    def free_energy(self, V):\n",
    "        return  -V.dot(self.b) - T.sum(T.log(1 + T.exp(V.dot(self.W) + self.c)), axis=1)\n",
    "\n",
    "    def sample_h_given_v(self, V):\n",
    "        p_h_given_v = T.nnet.sigmoid(V.dot(self.W) + self.c)\n",
    "        h_sample = self.rng.binomial(size=p_h_given_v.shape, n=1, p=p_h_given_v)\n",
    "        return h_sample\n",
    "\n",
    "    def sample_v_given_h(self, H):\n",
    "        p_v_given_h = T.nnet.sigmoid(H.dot(self.W.T) + self.b)\n",
    "        v_sample = self.rng.binomial(size=p_v_given_h.shape, n=1, p=p_v_given_h)\n",
    "        return v_sample\n",
    "\n",
    "    def forward_hidden(self, X):\n",
    "        return T.nnet.sigmoid(X.dot(self.W) + self.c)\n",
    "\n",
    "    def forward_output(self, X):\n",
    "        Z = self.forward_hidden(X)\n",
    "        Y = T.nnet.sigmoid(Z.dot(self.W.T) + self.b)\n",
    "        return Y\n",
    "\n",
    "    @staticmethod\n",
    "    def createFromArrays(W, c, b, an_id):\n",
    "        rbm = AutoEncoder(W.shape[1], an_id)\n",
    "        rbm.W = theano.shared(W, 'W_%s' % rbm.id)\n",
    "        rbm.c = theano.shared(c, 'c_%s' % rbm.id)\n",
    "        rbm.b = theano.shared(b, 'b_%s' % rbm.id)\n",
    "        rbm.params = [rbm.W, rbm.c, rbm.b]\n",
    "        rbm.forward_params = [rbm.W, rbm.c]\n",
    "        return rbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training rbm: 0\n",
      "epoch: 0\n",
      "j / n_batches: 0 / 410.0 cost: 151.3776596875377\n",
      "j / n_batches: 1 / 410.0 cost: 146.3203703878674\n",
      "j / n_batches: 2 / 410.0 cost: 144.0130903707576\n",
      "j / n_batches: 3 / 410.0 cost: 141.07134167507732\n",
      "j / n_batches: 4 / 410.0 cost: 131.94823864864358\n",
      "j / n_batches: 5 / 410.0 cost: 123.84549220651303\n",
      "j / n_batches: 6 / 410.0 cost: 119.59237113632864\n",
      "j / n_batches: 7 / 410.0 cost: 123.28984382905686\n",
      "j / n_batches: 8 / 410.0 cost: 118.12851764677872\n",
      "j / n_batches: 9 / 410.0 cost: 112.45371540827828\n",
      "j / n_batches: 10 / 410.0 cost: 110.9067863097084\n",
      "j / n_batches: 11 / 410.0 cost: 110.19866330603207\n",
      "j / n_batches: 12 / 410.0 cost: 110.03259367905912\n",
      "j / n_batches: 13 / 410.0 cost: 112.39211017898299\n",
      "j / n_batches: 14 / 410.0 cost: 113.12931414052512\n",
      "j / n_batches: 15 / 410.0 cost: 108.79558595584376\n",
      "j / n_batches: 16 / 410.0 cost: 107.86973196080658\n",
      "j / n_batches: 17 / 410.0 cost: 105.73634228737234\n",
      "j / n_batches: 18 / 410.0 cost: 103.64365574290981\n",
      "j / n_batches: 19 / 410.0 cost: 102.65084480521368\n",
      "j / n_batches: 20 / 410.0 cost: 102.48990928765538\n",
      "j / n_batches: 21 / 410.0 cost: 102.12505311916534\n",
      "j / n_batches: 22 / 410.0 cost: 100.36028769203105\n",
      "j / n_batches: 23 / 410.0 cost: 101.42896768063692\n",
      "j / n_batches: 24 / 410.0 cost: 102.01128874048509\n",
      "j / n_batches: 25 / 410.0 cost: 102.57346845106761\n",
      "j / n_batches: 26 / 410.0 cost: 98.54240410170145\n",
      "j / n_batches: 27 / 410.0 cost: 94.793740254612\n",
      "j / n_batches: 28 / 410.0 cost: 96.4194458473525\n",
      "j / n_batches: 29 / 410.0 cost: 94.60026655836103\n",
      "j / n_batches: 30 / 410.0 cost: 92.66180681321798\n",
      "j / n_batches: 31 / 410.0 cost: 90.08190786362361\n",
      "j / n_batches: 32 / 410.0 cost: 90.82954826412536\n",
      "j / n_batches: 33 / 410.0 cost: 91.18032152495674\n",
      "j / n_batches: 34 / 410.0 cost: 92.28782574501318\n",
      "j / n_batches: 35 / 410.0 cost: 92.63239986795547\n",
      "j / n_batches: 36 / 410.0 cost: 88.7879184283401\n",
      "j / n_batches: 37 / 410.0 cost: 85.97303960397016\n",
      "j / n_batches: 38 / 410.0 cost: 85.34528094920756\n",
      "j / n_batches: 39 / 410.0 cost: 86.14152553300487\n",
      "j / n_batches: 40 / 410.0 cost: 85.81230032168082\n",
      "j / n_batches: 41 / 410.0 cost: 84.0454884496328\n",
      "j / n_batches: 42 / 410.0 cost: 83.78322600614543\n",
      "j / n_batches: 43 / 410.0 cost: 83.76702462833795\n",
      "j / n_batches: 44 / 410.0 cost: 82.85490499269224\n",
      "j / n_batches: 45 / 410.0 cost: 81.82107533822452\n",
      "j / n_batches: 46 / 410.0 cost: 81.25653995022229\n",
      "j / n_batches: 47 / 410.0 cost: 81.50515431808796\n",
      "j / n_batches: 48 / 410.0 cost: 80.89484318319732\n",
      "j / n_batches: 49 / 410.0 cost: 79.81189855581157\n",
      "j / n_batches: 50 / 410.0 cost: 78.58236551592768\n",
      "j / n_batches: 51 / 410.0 cost: 79.99382679592873\n",
      "j / n_batches: 52 / 410.0 cost: 78.92239898944214\n",
      "j / n_batches: 53 / 410.0 cost: 77.6884568459571\n",
      "j / n_batches: 54 / 410.0 cost: 77.39514208017295\n",
      "j / n_batches: 55 / 410.0 cost: 77.14803895129907\n",
      "j / n_batches: 56 / 410.0 cost: 77.10328636240396\n",
      "j / n_batches: 57 / 410.0 cost: 76.84329861404301\n",
      "j / n_batches: 58 / 410.0 cost: 76.13903574219515\n",
      "j / n_batches: 59 / 410.0 cost: 75.64759198366802\n",
      "j / n_batches: 60 / 410.0 cost: 76.00313417644169\n",
      "j / n_batches: 61 / 410.0 cost: 76.2451171979586\n",
      "j / n_batches: 62 / 410.0 cost: 76.89035536752536\n",
      "j / n_batches: 63 / 410.0 cost: 74.68514485392267\n",
      "j / n_batches: 64 / 410.0 cost: 74.88430346142448\n",
      "j / n_batches: 65 / 410.0 cost: 75.18728474449208\n",
      "j / n_batches: 66 / 410.0 cost: 75.10692029371585\n",
      "j / n_batches: 67 / 410.0 cost: 74.32099989030935\n",
      "j / n_batches: 68 / 410.0 cost: 74.50556956269494\n",
      "j / n_batches: 69 / 410.0 cost: 73.8969558157525\n",
      "j / n_batches: 70 / 410.0 cost: 73.55031933357452\n",
      "j / n_batches: 71 / 410.0 cost: 73.35737566791092\n",
      "j / n_batches: 72 / 410.0 cost: 72.39862164052076\n",
      "j / n_batches: 73 / 410.0 cost: 71.98151735913791\n",
      "j / n_batches: 74 / 410.0 cost: 72.39229984051752\n",
      "j / n_batches: 75 / 410.0 cost: 72.03589800636026\n",
      "j / n_batches: 76 / 410.0 cost: 71.52073488856162\n",
      "j / n_batches: 77 / 410.0 cost: 70.57546377965006\n",
      "j / n_batches: 78 / 410.0 cost: 70.28292817146436\n",
      "j / n_batches: 79 / 410.0 cost: 70.4126154584682\n",
      "j / n_batches: 80 / 410.0 cost: 69.87195077732586\n",
      "j / n_batches: 81 / 410.0 cost: 70.21282453330231\n",
      "j / n_batches: 82 / 410.0 cost: 70.49262178584007\n",
      "j / n_batches: 83 / 410.0 cost: 69.73779972914286\n",
      "j / n_batches: 84 / 410.0 cost: 69.4003675804518\n",
      "j / n_batches: 85 / 410.0 cost: 69.17350755533698\n",
      "j / n_batches: 86 / 410.0 cost: 68.44149068909599\n",
      "j / n_batches: 87 / 410.0 cost: 68.41703551578418\n",
      "j / n_batches: 88 / 410.0 cost: 68.91637432880641\n",
      "j / n_batches: 89 / 410.0 cost: 68.2986972217035\n",
      "j / n_batches: 90 / 410.0 cost: 67.44941591695375\n",
      "j / n_batches: 91 / 410.0 cost: 68.03320608065064\n",
      "j / n_batches: 92 / 410.0 cost: 68.76191703327322\n",
      "j / n_batches: 93 / 410.0 cost: 67.47260463241848\n",
      "j / n_batches: 94 / 410.0 cost: 67.36189006887926\n",
      "j / n_batches: 95 / 410.0 cost: 67.10988190121316\n",
      "j / n_batches: 96 / 410.0 cost: 67.01273133422565\n",
      "j / n_batches: 97 / 410.0 cost: 66.70960304537606\n",
      "j / n_batches: 98 / 410.0 cost: 67.04296787036725\n",
      "j / n_batches: 99 / 410.0 cost: 66.36595529000436\n",
      "j / n_batches: 100 / 410.0 cost: 66.39399341264647\n",
      "j / n_batches: 101 / 410.0 cost: 66.5874601288127\n",
      "j / n_batches: 102 / 410.0 cost: 66.31365495044966\n",
      "j / n_batches: 103 / 410.0 cost: 66.1089257994081\n",
      "j / n_batches: 104 / 410.0 cost: 65.69138040162053\n",
      "j / n_batches: 105 / 410.0 cost: 65.8403584239477\n",
      "j / n_batches: 106 / 410.0 cost: 65.32509308678371\n",
      "j / n_batches: 107 / 410.0 cost: 65.29079145467195\n",
      "j / n_batches: 108 / 410.0 cost: 65.0592712630264\n",
      "j / n_batches: 109 / 410.0 cost: 65.39180177396224\n",
      "j / n_batches: 110 / 410.0 cost: 65.08828272863256\n",
      "j / n_batches: 111 / 410.0 cost: 65.05341533362696\n",
      "j / n_batches: 112 / 410.0 cost: 64.99538316001278\n",
      "j / n_batches: 113 / 410.0 cost: 64.42759926721362\n",
      "j / n_batches: 114 / 410.0 cost: 64.66133648207983\n",
      "j / n_batches: 115 / 410.0 cost: 64.09747331037408\n",
      "j / n_batches: 116 / 410.0 cost: 63.92316911921048\n",
      "j / n_batches: 117 / 410.0 cost: 63.9631893573749\n",
      "j / n_batches: 118 / 410.0 cost: 64.1086591391186\n",
      "j / n_batches: 119 / 410.0 cost: 64.01326628543995\n",
      "j / n_batches: 120 / 410.0 cost: 63.45089918980661\n",
      "j / n_batches: 121 / 410.0 cost: 63.22145646292291\n",
      "j / n_batches: 122 / 410.0 cost: 63.122208921868236\n",
      "j / n_batches: 123 / 410.0 cost: 62.972343254152186\n",
      "j / n_batches: 124 / 410.0 cost: 62.59390839777862\n",
      "j / n_batches: 125 / 410.0 cost: 62.6714100398691\n",
      "j / n_batches: 126 / 410.0 cost: 62.83067366601113\n",
      "j / n_batches: 127 / 410.0 cost: 62.37450998334174\n",
      "j / n_batches: 128 / 410.0 cost: 62.46334260556317\n",
      "j / n_batches: 129 / 410.0 cost: 62.46716269583546\n",
      "j / n_batches: 130 / 410.0 cost: 62.09039719546151\n",
      "j / n_batches: 131 / 410.0 cost: 61.9231096061737\n",
      "j / n_batches: 132 / 410.0 cost: 61.90394678559922\n",
      "j / n_batches: 133 / 410.0 cost: 61.756564124250346\n",
      "j / n_batches: 134 / 410.0 cost: 61.90098117137344\n",
      "j / n_batches: 135 / 410.0 cost: 61.44170654204548\n",
      "j / n_batches: 136 / 410.0 cost: 60.998480166496606\n",
      "j / n_batches: 137 / 410.0 cost: 60.97998848795834\n",
      "j / n_batches: 138 / 410.0 cost: 61.126141813113975\n",
      "j / n_batches: 139 / 410.0 cost: 61.02682066476232\n",
      "j / n_batches: 140 / 410.0 cost: 61.011467857826005\n",
      "j / n_batches: 141 / 410.0 cost: 61.29703186983515\n",
      "j / n_batches: 142 / 410.0 cost: 60.846955811274206\n",
      "j / n_batches: 143 / 410.0 cost: 60.51859360940882\n",
      "j / n_batches: 144 / 410.0 cost: 60.4186991499901\n",
      "j / n_batches: 145 / 410.0 cost: 60.31167783053003\n",
      "j / n_batches: 146 / 410.0 cost: 60.24743430503883\n",
      "j / n_batches: 147 / 410.0 cost: 60.37222216437856\n",
      "j / n_batches: 148 / 410.0 cost: 59.850572520235616\n",
      "j / n_batches: 149 / 410.0 cost: 59.77629987948818\n",
      "j / n_batches: 150 / 410.0 cost: 60.00982903496974\n",
      "j / n_batches: 151 / 410.0 cost: 59.842886612464575\n",
      "j / n_batches: 152 / 410.0 cost: 59.77229449556605\n",
      "j / n_batches: 153 / 410.0 cost: 59.38174364693127\n",
      "j / n_batches: 154 / 410.0 cost: 59.383341685069205\n",
      "j / n_batches: 155 / 410.0 cost: 59.16614840334437\n",
      "j / n_batches: 156 / 410.0 cost: 59.1943252541159\n",
      "j / n_batches: 157 / 410.0 cost: 58.95476159564448\n",
      "j / n_batches: 158 / 410.0 cost: 58.994439745641536\n",
      "j / n_batches: 159 / 410.0 cost: 58.817833569536205\n",
      "j / n_batches: 160 / 410.0 cost: 58.86601747475693\n",
      "j / n_batches: 161 / 410.0 cost: 58.543274910464966\n",
      "j / n_batches: 162 / 410.0 cost: 58.44625501975031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j / n_batches: 163 / 410.0 cost: 58.432566501640515\n",
      "j / n_batches: 164 / 410.0 cost: 58.424433173652176\n",
      "j / n_batches: 165 / 410.0 cost: 58.40426685857158\n",
      "j / n_batches: 166 / 410.0 cost: 58.471571640986475\n",
      "j / n_batches: 167 / 410.0 cost: 58.151797982513784\n",
      "j / n_batches: 168 / 410.0 cost: 58.0907241283887\n",
      "j / n_batches: 169 / 410.0 cost: 57.93790358601173\n",
      "j / n_batches: 170 / 410.0 cost: 58.18598168478066\n",
      "j / n_batches: 171 / 410.0 cost: 57.911122267277584\n",
      "j / n_batches: 172 / 410.0 cost: 57.60856387974265\n",
      "j / n_batches: 173 / 410.0 cost: 57.55097580549117\n",
      "j / n_batches: 174 / 410.0 cost: 57.70576115294002\n",
      "j / n_batches: 175 / 410.0 cost: 57.55174799777861\n",
      "j / n_batches: 176 / 410.0 cost: 57.60452997298541\n",
      "j / n_batches: 177 / 410.0 cost: 57.24867389403472\n",
      "j / n_batches: 178 / 410.0 cost: 57.28271915531148\n",
      "j / n_batches: 179 / 410.0 cost: 57.10263897193931\n",
      "j / n_batches: 180 / 410.0 cost: 57.17591303247873\n",
      "j / n_batches: 181 / 410.0 cost: 56.704042546667296\n",
      "j / n_batches: 182 / 410.0 cost: 56.936565896009704\n",
      "j / n_batches: 183 / 410.0 cost: 56.99780562773725\n",
      "j / n_batches: 184 / 410.0 cost: 56.71941634323164\n",
      "j / n_batches: 185 / 410.0 cost: 56.54146680652637\n",
      "j / n_batches: 186 / 410.0 cost: 56.43363317482188\n",
      "j / n_batches: 187 / 410.0 cost: 56.29080276212551\n",
      "j / n_batches: 188 / 410.0 cost: 56.68322035761971\n",
      "j / n_batches: 189 / 410.0 cost: 56.41081220926988\n",
      "j / n_batches: 190 / 410.0 cost: 56.17194420373685\n",
      "j / n_batches: 191 / 410.0 cost: 55.98459028579119\n",
      "j / n_batches: 192 / 410.0 cost: 56.099033976951844\n",
      "j / n_batches: 193 / 410.0 cost: 56.21160201000954\n",
      "j / n_batches: 194 / 410.0 cost: 56.031903628173026\n",
      "j / n_batches: 195 / 410.0 cost: 55.67330656030547\n",
      "j / n_batches: 196 / 410.0 cost: 55.66939997771639\n",
      "j / n_batches: 197 / 410.0 cost: 55.67968671075599\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest = getKaggleMNIST()\n",
    "dnn = DNN([1000, 750, 500], UnsupervisedModel=RBM)\n",
    "dnn.fit(Xtrain, Ytrain, Xtest, Ytest, epochs=3)\n",
    "\n",
    "# we compare with no pretraining in autoencoder.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM With TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the appropriate modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from util import getKaggleMNIST\n",
    "from autoencoder_tf import DNNclass RBM(object):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    def __init__(self, D, M, an_id):\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        self.id = an_id\n",
    "        self.build(D, M)\n",
    "\n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "\n",
    "    def build(self, D, M):\n",
    "        # params\n",
    "        self.W = tf.Variable(tf.random_normal(shape=(D, M)) * np.sqrt(2.0 / M))\n",
    "        # note: without limiting variance, you get numerical stability issues\n",
    "        self.c = tf.Variable(np.zeros(M).astype(np.float32))\n",
    "        self.b = tf.Variable(np.zeros(D).astype(np.float32))\n",
    "\n",
    "        # data\n",
    "        self.X_in = tf.placeholder(tf.float32, shape=(None, D))\n",
    "\n",
    "        # conditional probabilities\n",
    "        # NOTE: tf.contrib.distributions.Bernoulli API has changed in Tensorflow v1.2\n",
    "        V = self.X_in\n",
    "        p_h_given_v = tf.nn.sigmoid(tf.matmul(V, self.W) + self.c)\n",
    "        self.p_h_given_v = p_h_given_v # save for later\n",
    "        # self.rng_h_given_v = tf.contrib.distributions.Bernoulli(\n",
    "        #     probs=p_h_given_v,\n",
    "        #     dtype=tf.float32\n",
    "        # )\n",
    "        r = tf.random_uniform(shape=tf.shape(p_h_given_v))\n",
    "        H = tf.to_float(r < p_h_given_v)\n",
    "\n",
    "        p_v_given_h = tf.nn.sigmoid(tf.matmul(H, tf.transpose(self.W)) + self.b)\n",
    "        # self.rng_v_given_h = tf.contrib.distributions.Bernoulli(\n",
    "        #     probs=p_v_given_h,\n",
    "        #     dtype=tf.float32\n",
    "        # )\n",
    "        r = tf.random_uniform(shape=tf.shape(p_v_given_h))\n",
    "        X_sample = tf.to_float(r < p_v_given_h)\n",
    "\n",
    "\n",
    "        # build the objective\n",
    "        objective = tf.reduce_mean(self.free_energy(self.X_in)) - tf.reduce_mean(self.free_energy(X_sample))\n",
    "        self.train_op = tf.train.AdamOptimizer(1e-2).minimize(objective)\n",
    "        # self.train_op = tf.train.GradientDescentOptimizer(1e-3).minimize(objective)\n",
    "\n",
    "        # build the cost\n",
    "        # we won't use this to optimize the model parameters\n",
    "        # just to observe what happens during training\n",
    "        logits = self.forward_logits(self.X_in)\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=self.X_in,\n",
    "                logits=logits,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def fit(self, X, epochs=1, batch_sz=100, show_fig=False):\n",
    "        N, D = X.shape\n",
    "        n_batches = N // batch_sz\n",
    "\n",
    "        costs = []\n",
    "        print(\"training rbm: %s\" % self.id)\n",
    "        for i in range(epochs):\n",
    "            print(\"epoch:\", i)\n",
    "            X = shuffle(X)\n",
    "            for j in range(n_batches):\n",
    "                batch = X[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                _, c = self.session.run((self.train_op, self.cost), feed_dict={self.X_in: batch})\n",
    "                if j % 10 == 0:\n",
    "                    print(\"j / n_batches:\", j, \"/\", n_batches, \"cost:\", c)\n",
    "                costs.append(c)\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "    def free_energy(self, V):\n",
    "        b = tf.reshape(self.b, (self.D, 1))\n",
    "        first_term = -tf.matmul(V, b)\n",
    "        first_term = tf.reshape(first_term, (-1,))\n",
    "\n",
    "        second_term = -tf.reduce_sum(\n",
    "            # tf.log(1 + tf.exp(tf.matmul(V, self.W) + self.c)),\n",
    "            tf.nn.softplus(tf.matmul(V, self.W) + self.c),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        return first_term + second_term\n",
    "\n",
    "    def forward_hidden(self, X):\n",
    "        return tf.nn.sigmoid(tf.matmul(X, self.W) + self.c)\n",
    "\n",
    "    def forward_logits(self, X):\n",
    "        Z = self.forward_hidden(X)\n",
    "        return tf.matmul(Z, tf.transpose(self.W)) + self.b\n",
    "\n",
    "    def forward_output(self, X):\n",
    "        return tf.nn.sigmoid(self.forward_logits(X))\n",
    "\n",
    "    def transform(self, X):\n",
    "        # accepts and returns a real numpy array\n",
    "        # unlike forward_hidden and forward_output\n",
    "        # which deal with tensorflow variables\n",
    "        return self.session.run(self.p_h_given_v, feed_dict={self.X_in: X})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest = getKaggleMNIST()\n",
    "\n",
    "# same as autoencoder_tf.py\n",
    "Xtrain = Xtrain.astype(np.float32)\n",
    "Xtest = Xtest.astype(np.float32)\n",
    "_, D = Xtrain.shape\n",
    "K = len(set(Ytrain))\n",
    "dnn = DNN(D, [1000, 750, 500], K, UnsupervisedModel=RBM)\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init_op)\n",
    "    dnn.set_session(session)\n",
    "    dnn.fit(Xtrain, Ytrain, Xtest, Ytest, pretrain=True, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
