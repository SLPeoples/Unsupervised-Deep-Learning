{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the appropriate modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autoencoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6cfec77d5d5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared_randomstreams\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomStreams\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetKaggleMNIST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'autoencoder'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from util import relu, error_rate, getKaggleMNIST, init_weights\n",
    "from autoencoder import DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM With TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://deeplearningcourses.com/c/unsupervised-deep-learning-in-python\n",
    "# https://www.udemy.com/unsupervised-deep-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from util import getKaggleMNIST\n",
    "from autoencoder_tf import DNN\n",
    "\n",
    "\n",
    "class RBM(object):\n",
    "    def __init__(self, D, M, an_id):\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        self.id = an_id\n",
    "        self.build(D, M)\n",
    "\n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "\n",
    "    def build(self, D, M):\n",
    "        # params\n",
    "        self.W = tf.Variable(tf.random_normal(shape=(D, M)) * np.sqrt(2.0 / M))\n",
    "        # note: without limiting variance, you get numerical stability issues\n",
    "        self.c = tf.Variable(np.zeros(M).astype(np.float32))\n",
    "        self.b = tf.Variable(np.zeros(D).astype(np.float32))\n",
    "\n",
    "        # data\n",
    "        self.X_in = tf.placeholder(tf.float32, shape=(None, D))\n",
    "\n",
    "        # conditional probabilities\n",
    "        # NOTE: tf.contrib.distributions.Bernoulli API has changed in Tensorflow v1.2\n",
    "        V = self.X_in\n",
    "        p_h_given_v = tf.nn.sigmoid(tf.matmul(V, self.W) + self.c)\n",
    "        self.p_h_given_v = p_h_given_v # save for later\n",
    "        # self.rng_h_given_v = tf.contrib.distributions.Bernoulli(\n",
    "        #     probs=p_h_given_v,\n",
    "        #     dtype=tf.float32\n",
    "        # )\n",
    "        r = tf.random_uniform(shape=tf.shape(p_h_given_v))\n",
    "        H = tf.to_float(r < p_h_given_v)\n",
    "\n",
    "        p_v_given_h = tf.nn.sigmoid(tf.matmul(H, tf.transpose(self.W)) + self.b)\n",
    "        # self.rng_v_given_h = tf.contrib.distributions.Bernoulli(\n",
    "        #     probs=p_v_given_h,\n",
    "        #     dtype=tf.float32\n",
    "        # )\n",
    "        r = tf.random_uniform(shape=tf.shape(p_v_given_h))\n",
    "        X_sample = tf.to_float(r < p_v_given_h)\n",
    "\n",
    "\n",
    "        # build the objective\n",
    "        objective = tf.reduce_mean(self.free_energy(self.X_in)) - tf.reduce_mean(self.free_energy(X_sample))\n",
    "        self.train_op = tf.train.AdamOptimizer(1e-2).minimize(objective)\n",
    "        # self.train_op = tf.train.GradientDescentOptimizer(1e-3).minimize(objective)\n",
    "\n",
    "        # build the cost\n",
    "        # we won't use this to optimize the model parameters\n",
    "        # just to observe what happens during training\n",
    "        logits = self.forward_logits(self.X_in)\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=self.X_in,\n",
    "                logits=logits,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def fit(self, X, epochs=1, batch_sz=100, show_fig=False):\n",
    "        N, D = X.shape\n",
    "        n_batches = N // batch_sz\n",
    "\n",
    "        costs = []\n",
    "        print(\"training rbm: %s\" % self.id)\n",
    "        for i in range(epochs):\n",
    "            print(\"epoch:\", i)\n",
    "            X = shuffle(X)\n",
    "            for j in range(n_batches):\n",
    "                batch = X[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                _, c = self.session.run((self.train_op, self.cost), feed_dict={self.X_in: batch})\n",
    "                if j % 10 == 0:\n",
    "                    print(\"j / n_batches:\", j, \"/\", n_batches, \"cost:\", c)\n",
    "                costs.append(c)\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "    def free_energy(self, V):\n",
    "        b = tf.reshape(self.b, (self.D, 1))\n",
    "        first_term = -tf.matmul(V, b)\n",
    "        first_term = tf.reshape(first_term, (-1,))\n",
    "\n",
    "        second_term = -tf.reduce_sum(\n",
    "            # tf.log(1 + tf.exp(tf.matmul(V, self.W) + self.c)),\n",
    "            tf.nn.softplus(tf.matmul(V, self.W) + self.c),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        return first_term + second_term\n",
    "\n",
    "    def forward_hidden(self, X):\n",
    "        return tf.nn.sigmoid(tf.matmul(X, self.W) + self.c)\n",
    "\n",
    "    def forward_logits(self, X):\n",
    "        Z = self.forward_hidden(X)\n",
    "        return tf.matmul(Z, tf.transpose(self.W)) + self.b\n",
    "\n",
    "    def forward_output(self, X):\n",
    "        return tf.nn.sigmoid(self.forward_logits(X))\n",
    "\n",
    "    def transform(self, X):\n",
    "        # accepts and returns a real numpy array\n",
    "        # unlike forward_hidden and forward_output\n",
    "        # which deal with tensorflow variables\n",
    "        return self.session.run(self.p_h_given_v, feed_dict={self.X_in: X})\n",
    "\n",
    "\n",
    "def main():\n",
    "    Xtrain, Ytrain, Xtest, Ytest = getKaggleMNIST()\n",
    "\n",
    "    # same as autoencoder_tf.py\n",
    "    Xtrain = Xtrain.astype(np.float32)\n",
    "    Xtest = Xtest.astype(np.float32)\n",
    "    _, D = Xtrain.shape\n",
    "    K = len(set(Ytrain))\n",
    "    dnn = DNN(D, [1000, 750, 500], K, UnsupervisedModel=RBM)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init_op)\n",
    "        dnn.set_session(session)\n",
    "        dnn.fit(Xtrain, Ytrain, Xtest, Ytest, pretrain=True, epochs=10)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
