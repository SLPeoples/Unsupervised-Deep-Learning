{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the appropriate modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from util import relu, error_rate, getKaggleMNIST, init_weights\n",
    "from autoencoder import DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    def __init__(self, M, an_id):\n",
    "        self.M = M\n",
    "        self.id = an_id\n",
    "        self.rng = RandomStreams()\n",
    "\n",
    "    def fit(self, X, learning_rate=0.1, epochs=1, batch_sz=100, show_fig=False):\n",
    "        N, D = X.shape\n",
    "        n_batches = N / batch_sz\n",
    "\n",
    "        W0 = init_weights((D, self.M))\n",
    "        self.W = theano.shared(W0, 'W_%s' % self.id)\n",
    "        self.c = theano.shared(np.zeros(self.M), 'c_%s' % self.id)\n",
    "        self.b = theano.shared(np.zeros(D), 'b_%s' % self.id)\n",
    "        self.params = [self.W, self.c, self.b]\n",
    "        self.forward_params = [self.W, self.c]\n",
    "\n",
    "        # we won't use this to fit the RBM but we will use these for backpropagation later\n",
    "        # TODO: technically they should be reset before doing backprop\n",
    "        self.dW = theano.shared(np.zeros(W0.shape), 'dW_%s' % self.id)\n",
    "        self.dc = theano.shared(np.zeros(self.M), 'dbh_%s' % self.id)\n",
    "        self.db = theano.shared(np.zeros(D), 'dbo_%s' % self.id)\n",
    "        self.dparams = [self.dW, self.dc, self.db]\n",
    "        self.forward_dparams = [self.dW, self.dc]\n",
    "\n",
    "        X_in = T.matrix('X_%s' % self.id)\n",
    "\n",
    "        # attach it to the object so it can be used later\n",
    "        # must be sigmoidal because the output is also a sigmoid\n",
    "        H = T.nnet.sigmoid(X_in.dot(self.W) + self.c)\n",
    "        self.hidden_op = theano.function(\n",
    "            inputs=[X_in],\n",
    "            outputs=H,\n",
    "        )\n",
    "\n",
    "        # we won't use this cost to do any updates\n",
    "        # but we would like to see how this cost function changes\n",
    "        # as we do contrastive divergence\n",
    "        X_hat = self.forward_output(X_in)\n",
    "        cost = -(X_in * T.log(X_hat) + (1 - X_in) * T.log(1 - X_hat)).sum() / (batch_sz * D)\n",
    "        cost_op = theano.function(\n",
    "            inputs=[X_in],\n",
    "            outputs=cost,\n",
    "        )\n",
    "\n",
    "        # do one round of Gibbs sampling to obtain X_sample\n",
    "        H = self.sample_h_given_v(X_in)\n",
    "        X_sample = self.sample_v_given_h(H)\n",
    "\n",
    "        # define the objective, updates, and train function\n",
    "        objective = T.mean(self.free_energy(X_in)) - T.mean(self.free_energy(X_sample))\n",
    "\n",
    "        # need to consider X_sample constant because you can't take the gradient of random numbers in Theano\n",
    "        updates = [(p, p - learning_rate*T.grad(\n",
    "            objective, p, consider_constant=[X_sample])) for p in self.params]\n",
    "        train_op = theano.function(\n",
    "            inputs=[X_in],\n",
    "            updates=updates,\n",
    "        )\n",
    "\n",
    "        costs = []\n",
    "        print (\"training rbm: %s\" % self.id)\n",
    "        for i in range(int(epochs)):\n",
    "            print (\"epoch:\", i)\n",
    "            X = shuffle(X)\n",
    "            for j in range(int(n_batches)):\n",
    "                batch = X[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                train_op(batch)\n",
    "                the_cost = cost_op(X)  # technically we could also get the cost for Xtest here\n",
    "                print (\"j / n_batches:\", j, \"/\", n_batches, \"cost:\", the_cost)\n",
    "                costs.append(the_cost)\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "    def free_energy(self, V):\n",
    "        return  -V.dot(self.b) - T.sum(T.log(1 + T.exp(V.dot(self.W) + self.c)), axis=1)\n",
    "\n",
    "    def sample_h_given_v(self, V):\n",
    "        p_h_given_v = T.nnet.sigmoid(V.dot(self.W) + self.c)\n",
    "        h_sample = self.rng.binomial(size=p_h_given_v.shape, n=1, p=p_h_given_v)\n",
    "        return h_sample\n",
    "\n",
    "    def sample_v_given_h(self, H):\n",
    "        p_v_given_h = T.nnet.sigmoid(H.dot(self.W.T) + self.b)\n",
    "        v_sample = self.rng.binomial(size=p_v_given_h.shape, n=1, p=p_v_given_h)\n",
    "        return v_sample\n",
    "\n",
    "    def forward_hidden(self, X):\n",
    "        return T.nnet.sigmoid(X.dot(self.W) + self.c)\n",
    "\n",
    "    def forward_output(self, X):\n",
    "        Z = self.forward_hidden(X)\n",
    "        Y = T.nnet.sigmoid(Z.dot(self.W.T) + self.b)\n",
    "        return Y\n",
    "\n",
    "    @staticmethod\n",
    "    def createFromArrays(W, c, b, an_id):\n",
    "        rbm = AutoEncoder(W.shape[1], an_id)\n",
    "        rbm.W = theano.shared(W, 'W_%s' % rbm.id)\n",
    "        rbm.c = theano.shared(c, 'c_%s' % rbm.id)\n",
    "        rbm.b = theano.shared(b, 'b_%s' % rbm.id)\n",
    "        rbm.params = [rbm.W, rbm.c, rbm.b]\n",
    "        rbm.forward_params = [rbm.W, rbm.c]\n",
    "        return rbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training rbm: 0\n",
      "epoch: 0\n",
      "j / n_batches: 0 / 410.0 cost: 151.3776596875377\n",
      "j / n_batches: 1 / 410.0 cost: 146.3203703878674\n",
      "j / n_batches: 2 / 410.0 cost: 144.0130903707576\n",
      "j / n_batches: 3 / 410.0 cost: 141.07134167507732\n",
      "j / n_batches: 4 / 410.0 cost: 131.94823864864358\n",
      "j / n_batches: 5 / 410.0 cost: 123.84549220651303\n",
      "j / n_batches: 6 / 410.0 cost: 119.59237113632864\n",
      "j / n_batches: 7 / 410.0 cost: 123.28984382905686\n",
      "j / n_batches: 8 / 410.0 cost: 118.12851764677872\n",
      "j / n_batches: 9 / 410.0 cost: 112.45371540827828\n",
      "j / n_batches: 10 / 410.0 cost: 110.9067863097084\n",
      "j / n_batches: 11 / 410.0 cost: 110.19866330603207\n",
      "j / n_batches: 12 / 410.0 cost: 110.03259367905912\n",
      "j / n_batches: 13 / 410.0 cost: 112.39211017898299\n",
      "j / n_batches: 14 / 410.0 cost: 113.12931414052512\n",
      "j / n_batches: 15 / 410.0 cost: 108.79558595584376\n",
      "j / n_batches: 16 / 410.0 cost: 107.86973196080658\n",
      "j / n_batches: 17 / 410.0 cost: 105.73634228737234\n",
      "j / n_batches: 18 / 410.0 cost: 103.64365574290981\n",
      "j / n_batches: 19 / 410.0 cost: 102.65084480521368\n",
      "j / n_batches: 20 / 410.0 cost: 102.48990928765538\n",
      "j / n_batches: 21 / 410.0 cost: 102.12505311916534\n",
      "j / n_batches: 22 / 410.0 cost: 100.36028769203105\n",
      "j / n_batches: 23 / 410.0 cost: 101.42896768063692\n",
      "j / n_batches: 24 / 410.0 cost: 102.01128874048509\n",
      "j / n_batches: 25 / 410.0 cost: 102.57346845106761\n",
      "j / n_batches: 26 / 410.0 cost: 98.54240410170145\n",
      "j / n_batches: 27 / 410.0 cost: 94.793740254612\n",
      "j / n_batches: 28 / 410.0 cost: 96.4194458473525\n",
      "j / n_batches: 29 / 410.0 cost: 94.60026655836103\n",
      "j / n_batches: 30 / 410.0 cost: 92.66180681321798\n",
      "j / n_batches: 31 / 410.0 cost: 90.08190786362361\n",
      "j / n_batches: 32 / 410.0 cost: 90.82954826412536\n",
      "j / n_batches: 33 / 410.0 cost: 91.18032152495674\n",
      "j / n_batches: 34 / 410.0 cost: 92.28782574501318\n",
      "j / n_batches: 35 / 410.0 cost: 92.63239986795547\n",
      "j / n_batches: 36 / 410.0 cost: 88.7879184283401\n",
      "j / n_batches: 37 / 410.0 cost: 85.97303960397016\n",
      "j / n_batches: 38 / 410.0 cost: 85.34528094920756\n",
      "j / n_batches: 39 / 410.0 cost: 86.14152553300487\n",
      "j / n_batches: 40 / 410.0 cost: 85.81230032168082\n",
      "j / n_batches: 41 / 410.0 cost: 84.0454884496328\n",
      "j / n_batches: 42 / 410.0 cost: 83.78322600614543\n",
      "j / n_batches: 43 / 410.0 cost: 83.76702462833795\n",
      "j / n_batches: 44 / 410.0 cost: 82.85490499269224\n",
      "j / n_batches: 45 / 410.0 cost: 81.82107533822452\n",
      "j / n_batches: 46 / 410.0 cost: 81.25653995022229\n",
      "j / n_batches: 47 / 410.0 cost: 81.50515431808796\n",
      "j / n_batches: 48 / 410.0 cost: 80.89484318319732\n",
      "j / n_batches: 49 / 410.0 cost: 79.81189855581157\n",
      "j / n_batches: 50 / 410.0 cost: 78.58236551592768\n",
      "j / n_batches: 51 / 410.0 cost: 79.99382679592873\n",
      "j / n_batches: 52 / 410.0 cost: 78.92239898944214\n",
      "j / n_batches: 53 / 410.0 cost: 77.6884568459571\n",
      "j / n_batches: 54 / 410.0 cost: 77.39514208017295\n",
      "j / n_batches: 55 / 410.0 cost: 77.14803895129907\n",
      "j / n_batches: 56 / 410.0 cost: 77.10328636240396\n",
      "j / n_batches: 57 / 410.0 cost: 76.84329861404301\n",
      "j / n_batches: 58 / 410.0 cost: 76.13903574219515\n",
      "j / n_batches: 59 / 410.0 cost: 75.64759198366802\n",
      "j / n_batches: 60 / 410.0 cost: 76.00313417644169\n",
      "j / n_batches: 61 / 410.0 cost: 76.2451171979586\n",
      "j / n_batches: 62 / 410.0 cost: 76.89035536752536\n",
      "j / n_batches: 63 / 410.0 cost: 74.68514485392267\n",
      "j / n_batches: 64 / 410.0 cost: 74.88430346142448\n",
      "j / n_batches: 65 / 410.0 cost: 75.18728474449208\n",
      "j / n_batches: 66 / 410.0 cost: 75.10692029371585\n",
      "j / n_batches: 67 / 410.0 cost: 74.32099989030935\n",
      "j / n_batches: 68 / 410.0 cost: 74.50556956269494\n",
      "j / n_batches: 69 / 410.0 cost: 73.8969558157525\n",
      "j / n_batches: 70 / 410.0 cost: 73.55031933357452\n",
      "j / n_batches: 71 / 410.0 cost: 73.35737566791092\n",
      "j / n_batches: 72 / 410.0 cost: 72.39862164052076\n",
      "j / n_batches: 73 / 410.0 cost: 71.98151735913791\n",
      "j / n_batches: 74 / 410.0 cost: 72.39229984051752\n",
      "j / n_batches: 75 / 410.0 cost: 72.03589800636026\n",
      "j / n_batches: 76 / 410.0 cost: 71.52073488856162\n",
      "j / n_batches: 77 / 410.0 cost: 70.57546377965006\n",
      "j / n_batches: 78 / 410.0 cost: 70.28292817146436\n",
      "j / n_batches: 79 / 410.0 cost: 70.4126154584682\n",
      "j / n_batches: 80 / 410.0 cost: 69.87195077732586\n",
      "j / n_batches: 81 / 410.0 cost: 70.21282453330231\n",
      "j / n_batches: 82 / 410.0 cost: 70.49262178584007\n",
      "j / n_batches: 83 / 410.0 cost: 69.73779972914286\n",
      "j / n_batches: 84 / 410.0 cost: 69.4003675804518\n",
      "j / n_batches: 85 / 410.0 cost: 69.17350755533698\n",
      "j / n_batches: 86 / 410.0 cost: 68.44149068909599\n",
      "j / n_batches: 87 / 410.0 cost: 68.41703551578418\n",
      "j / n_batches: 88 / 410.0 cost: 68.91637432880641\n",
      "j / n_batches: 89 / 410.0 cost: 68.2986972217035\n",
      "j / n_batches: 90 / 410.0 cost: 67.44941591695375\n",
      "j / n_batches: 91 / 410.0 cost: 68.03320608065064\n",
      "j / n_batches: 92 / 410.0 cost: 68.76191703327322\n",
      "j / n_batches: 93 / 410.0 cost: 67.47260463241848\n",
      "j / n_batches: 94 / 410.0 cost: 67.36189006887926\n",
      "j / n_batches: 95 / 410.0 cost: 67.10988190121316\n",
      "j / n_batches: 96 / 410.0 cost: 67.01273133422565\n",
      "j / n_batches: 97 / 410.0 cost: 66.70960304537606\n",
      "j / n_batches: 98 / 410.0 cost: 67.04296787036725\n",
      "j / n_batches: 99 / 410.0 cost: 66.36595529000436\n",
      "j / n_batches: 100 / 410.0 cost: 66.39399341264647\n",
      "j / n_batches: 101 / 410.0 cost: 66.5874601288127\n",
      "j / n_batches: 102 / 410.0 cost: 66.31365495044966\n",
      "j / n_batches: 103 / 410.0 cost: 66.1089257994081\n",
      "j / n_batches: 104 / 410.0 cost: 65.69138040162053\n",
      "j / n_batches: 105 / 410.0 cost: 65.8403584239477\n",
      "j / n_batches: 106 / 410.0 cost: 65.32509308678371\n",
      "j / n_batches: 107 / 410.0 cost: 65.29079145467195\n",
      "j / n_batches: 108 / 410.0 cost: 65.0592712630264\n",
      "j / n_batches: 109 / 410.0 cost: 65.39180177396224\n",
      "j / n_batches: 110 / 410.0 cost: 65.08828272863256\n",
      "j / n_batches: 111 / 410.0 cost: 65.05341533362696\n",
      "j / n_batches: 112 / 410.0 cost: 64.99538316001278\n",
      "j / n_batches: 113 / 410.0 cost: 64.42759926721362\n",
      "j / n_batches: 114 / 410.0 cost: 64.66133648207983\n",
      "j / n_batches: 115 / 410.0 cost: 64.09747331037408\n",
      "j / n_batches: 116 / 410.0 cost: 63.92316911921048\n",
      "j / n_batches: 117 / 410.0 cost: 63.9631893573749\n",
      "j / n_batches: 118 / 410.0 cost: 64.1086591391186\n",
      "j / n_batches: 119 / 410.0 cost: 64.01326628543995\n",
      "j / n_batches: 120 / 410.0 cost: 63.45089918980661\n",
      "j / n_batches: 121 / 410.0 cost: 63.22145646292291\n",
      "j / n_batches: 122 / 410.0 cost: 63.122208921868236\n",
      "j / n_batches: 123 / 410.0 cost: 62.972343254152186\n",
      "j / n_batches: 124 / 410.0 cost: 62.59390839777862\n",
      "j / n_batches: 125 / 410.0 cost: 62.6714100398691\n",
      "j / n_batches: 126 / 410.0 cost: 62.83067366601113\n",
      "j / n_batches: 127 / 410.0 cost: 62.37450998334174\n",
      "j / n_batches: 128 / 410.0 cost: 62.46334260556317\n",
      "j / n_batches: 129 / 410.0 cost: 62.46716269583546\n",
      "j / n_batches: 130 / 410.0 cost: 62.09039719546151\n",
      "j / n_batches: 131 / 410.0 cost: 61.9231096061737\n",
      "j / n_batches: 132 / 410.0 cost: 61.90394678559922\n",
      "j / n_batches: 133 / 410.0 cost: 61.756564124250346\n",
      "j / n_batches: 134 / 410.0 cost: 61.90098117137344\n",
      "j / n_batches: 135 / 410.0 cost: 61.44170654204548\n",
      "j / n_batches: 136 / 410.0 cost: 60.998480166496606\n",
      "j / n_batches: 137 / 410.0 cost: 60.97998848795834\n",
      "j / n_batches: 138 / 410.0 cost: 61.126141813113975\n",
      "j / n_batches: 139 / 410.0 cost: 61.02682066476232\n",
      "j / n_batches: 140 / 410.0 cost: 61.011467857826005\n",
      "j / n_batches: 141 / 410.0 cost: 61.29703186983515\n",
      "j / n_batches: 142 / 410.0 cost: 60.846955811274206\n",
      "j / n_batches: 143 / 410.0 cost: 60.51859360940882\n",
      "j / n_batches: 144 / 410.0 cost: 60.4186991499901\n",
      "j / n_batches: 145 / 410.0 cost: 60.31167783053003\n",
      "j / n_batches: 146 / 410.0 cost: 60.24743430503883\n",
      "j / n_batches: 147 / 410.0 cost: 60.37222216437856\n",
      "j / n_batches: 148 / 410.0 cost: 59.850572520235616\n",
      "j / n_batches: 149 / 410.0 cost: 59.77629987948818\n",
      "j / n_batches: 150 / 410.0 cost: 60.00982903496974\n",
      "j / n_batches: 151 / 410.0 cost: 59.842886612464575\n",
      "j / n_batches: 152 / 410.0 cost: 59.77229449556605\n",
      "j / n_batches: 153 / 410.0 cost: 59.38174364693127\n",
      "j / n_batches: 154 / 410.0 cost: 59.383341685069205\n",
      "j / n_batches: 155 / 410.0 cost: 59.16614840334437\n",
      "j / n_batches: 156 / 410.0 cost: 59.1943252541159\n",
      "j / n_batches: 157 / 410.0 cost: 58.95476159564448\n",
      "j / n_batches: 158 / 410.0 cost: 58.994439745641536\n",
      "j / n_batches: 159 / 410.0 cost: 58.817833569536205\n",
      "j / n_batches: 160 / 410.0 cost: 58.86601747475693\n",
      "j / n_batches: 161 / 410.0 cost: 58.543274910464966\n",
      "j / n_batches: 162 / 410.0 cost: 58.44625501975031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j / n_batches: 163 / 410.0 cost: 58.432566501640515\n",
      "j / n_batches: 164 / 410.0 cost: 58.424433173652176\n",
      "j / n_batches: 165 / 410.0 cost: 58.40426685857158\n",
      "j / n_batches: 166 / 410.0 cost: 58.471571640986475\n",
      "j / n_batches: 167 / 410.0 cost: 58.151797982513784\n",
      "j / n_batches: 168 / 410.0 cost: 58.0907241283887\n",
      "j / n_batches: 169 / 410.0 cost: 57.93790358601173\n",
      "j / n_batches: 170 / 410.0 cost: 58.18598168478066\n",
      "j / n_batches: 171 / 410.0 cost: 57.911122267277584\n",
      "j / n_batches: 172 / 410.0 cost: 57.60856387974265\n",
      "j / n_batches: 173 / 410.0 cost: 57.55097580549117\n",
      "j / n_batches: 174 / 410.0 cost: 57.70576115294002\n",
      "j / n_batches: 175 / 410.0 cost: 57.55174799777861\n",
      "j / n_batches: 176 / 410.0 cost: 57.60452997298541\n",
      "j / n_batches: 177 / 410.0 cost: 57.24867389403472\n",
      "j / n_batches: 178 / 410.0 cost: 57.28271915531148\n",
      "j / n_batches: 179 / 410.0 cost: 57.10263897193931\n",
      "j / n_batches: 180 / 410.0 cost: 57.17591303247873\n",
      "j / n_batches: 181 / 410.0 cost: 56.704042546667296\n",
      "j / n_batches: 182 / 410.0 cost: 56.936565896009704\n",
      "j / n_batches: 183 / 410.0 cost: 56.99780562773725\n",
      "j / n_batches: 184 / 410.0 cost: 56.71941634323164\n",
      "j / n_batches: 185 / 410.0 cost: 56.54146680652637\n",
      "j / n_batches: 186 / 410.0 cost: 56.43363317482188\n",
      "j / n_batches: 187 / 410.0 cost: 56.29080276212551\n",
      "j / n_batches: 188 / 410.0 cost: 56.68322035761971\n",
      "j / n_batches: 189 / 410.0 cost: 56.41081220926988\n",
      "j / n_batches: 190 / 410.0 cost: 56.17194420373685\n",
      "j / n_batches: 191 / 410.0 cost: 55.98459028579119\n",
      "j / n_batches: 192 / 410.0 cost: 56.099033976951844\n",
      "j / n_batches: 193 / 410.0 cost: 56.21160201000954\n",
      "j / n_batches: 194 / 410.0 cost: 56.031903628173026\n",
      "j / n_batches: 195 / 410.0 cost: 55.67330656030547\n",
      "j / n_batches: 196 / 410.0 cost: 55.66939997771639\n",
      "j / n_batches: 197 / 410.0 cost: 55.67968671075599\n",
      "j / n_batches: 198 / 410.0 cost: 55.47743487261543\n",
      "j / n_batches: 199 / 410.0 cost: 55.48974655260648\n",
      "j / n_batches: 200 / 410.0 cost: 55.328474033387984\n",
      "j / n_batches: 201 / 410.0 cost: 55.363938741657385\n",
      "j / n_batches: 202 / 410.0 cost: 55.70077875520975\n",
      "j / n_batches: 203 / 410.0 cost: 55.268142989436164\n",
      "j / n_batches: 204 / 410.0 cost: 55.517031414949194\n",
      "j / n_batches: 205 / 410.0 cost: 55.1360137784741\n",
      "j / n_batches: 206 / 410.0 cost: 55.054115068214955\n",
      "j / n_batches: 207 / 410.0 cost: 55.17143930302245\n",
      "j / n_batches: 208 / 410.0 cost: 54.850200628271736\n",
      "j / n_batches: 209 / 410.0 cost: 54.70734795050731\n",
      "j / n_batches: 210 / 410.0 cost: 54.82239955072358\n",
      "j / n_batches: 211 / 410.0 cost: 54.7802188401481\n",
      "j / n_batches: 212 / 410.0 cost: 54.76148138295757\n",
      "j / n_batches: 213 / 410.0 cost: 54.80588724233265\n",
      "j / n_batches: 214 / 410.0 cost: 54.47775124350975\n",
      "j / n_batches: 215 / 410.0 cost: 54.54269563790579\n",
      "j / n_batches: 216 / 410.0 cost: 54.51898412692944\n",
      "j / n_batches: 217 / 410.0 cost: 54.49232749846486\n",
      "j / n_batches: 218 / 410.0 cost: 54.25705020624838\n",
      "j / n_batches: 219 / 410.0 cost: 54.39962435299657\n",
      "j / n_batches: 220 / 410.0 cost: 53.947153074187725\n",
      "j / n_batches: 221 / 410.0 cost: 53.98727452345181\n",
      "j / n_batches: 222 / 410.0 cost: 53.988376712254286\n",
      "j / n_batches: 223 / 410.0 cost: 53.71208813910963\n",
      "j / n_batches: 224 / 410.0 cost: 53.783474175265106\n",
      "j / n_batches: 225 / 410.0 cost: 53.7473780214857\n",
      "j / n_batches: 226 / 410.0 cost: 53.76397963218782\n",
      "j / n_batches: 227 / 410.0 cost: 54.14587250832246\n",
      "j / n_batches: 228 / 410.0 cost: 53.53646245656824\n",
      "j / n_batches: 229 / 410.0 cost: 53.47386633296192\n",
      "j / n_batches: 230 / 410.0 cost: 53.493814148067024\n",
      "j / n_batches: 231 / 410.0 cost: 53.36989903824535\n",
      "j / n_batches: 232 / 410.0 cost: 53.4884004682271\n",
      "j / n_batches: 233 / 410.0 cost: 53.152709160917105\n",
      "j / n_batches: 234 / 410.0 cost: 53.358483988831445\n",
      "j / n_batches: 235 / 410.0 cost: 53.117720591222785\n",
      "j / n_batches: 236 / 410.0 cost: 53.031211391278404\n",
      "j / n_batches: 237 / 410.0 cost: 53.00793925699093\n",
      "j / n_batches: 238 / 410.0 cost: 53.12513264762684\n",
      "j / n_batches: 239 / 410.0 cost: 52.93971943361732\n",
      "j / n_batches: 240 / 410.0 cost: 52.97082457007627\n",
      "j / n_batches: 241 / 410.0 cost: 52.98742693531357\n",
      "j / n_batches: 242 / 410.0 cost: 52.83296723055943\n",
      "j / n_batches: 243 / 410.0 cost: 52.50328256220878\n",
      "j / n_batches: 244 / 410.0 cost: 52.62621964341052\n",
      "j / n_batches: 245 / 410.0 cost: 52.613966617511245\n",
      "j / n_batches: 246 / 410.0 cost: 52.4910568080472\n",
      "j / n_batches: 247 / 410.0 cost: 52.66229856280389\n",
      "j / n_batches: 248 / 410.0 cost: 52.6058590602216\n",
      "j / n_batches: 249 / 410.0 cost: 52.39463576371327\n",
      "j / n_batches: 250 / 410.0 cost: 52.4832488339454\n",
      "j / n_batches: 251 / 410.0 cost: 52.42522173027332\n",
      "j / n_batches: 252 / 410.0 cost: 52.3196539211233\n",
      "j / n_batches: 253 / 410.0 cost: 52.34467005491518\n",
      "j / n_batches: 254 / 410.0 cost: 52.16917468795071\n",
      "j / n_batches: 255 / 410.0 cost: 52.28681044296655\n",
      "j / n_batches: 256 / 410.0 cost: 52.030195468966404\n",
      "j / n_batches: 257 / 410.0 cost: 51.79947280131288\n",
      "j / n_batches: 258 / 410.0 cost: 51.734632099323484\n",
      "j / n_batches: 259 / 410.0 cost: 51.82818287449247\n",
      "j / n_batches: 260 / 410.0 cost: 51.7415570118584\n",
      "j / n_batches: 261 / 410.0 cost: 51.600030773726196\n",
      "j / n_batches: 262 / 410.0 cost: 51.88260264686951\n",
      "j / n_batches: 263 / 410.0 cost: 51.54040730469795\n",
      "j / n_batches: 264 / 410.0 cost: 51.60282574005285\n",
      "j / n_batches: 265 / 410.0 cost: 51.58113224404891\n",
      "j / n_batches: 266 / 410.0 cost: 51.320638208998\n",
      "j / n_batches: 267 / 410.0 cost: 51.60238670735033\n",
      "j / n_batches: 268 / 410.0 cost: 51.523994429414955\n",
      "j / n_batches: 269 / 410.0 cost: 51.50126508218576\n",
      "j / n_batches: 270 / 410.0 cost: 51.34152850775843\n",
      "j / n_batches: 271 / 410.0 cost: 51.368294804541456\n",
      "j / n_batches: 272 / 410.0 cost: 51.16850545153071\n",
      "j / n_batches: 273 / 410.0 cost: 51.367323165858686\n",
      "j / n_batches: 274 / 410.0 cost: 50.96518986557511\n",
      "j / n_batches: 275 / 410.0 cost: 51.110371485486034\n",
      "j / n_batches: 276 / 410.0 cost: 51.2886392556261\n",
      "j / n_batches: 277 / 410.0 cost: 51.18073150230208\n",
      "j / n_batches: 278 / 410.0 cost: 51.01727059879746\n",
      "j / n_batches: 279 / 410.0 cost: 50.82340366119247\n",
      "j / n_batches: 280 / 410.0 cost: 50.89620125821198\n",
      "j / n_batches: 281 / 410.0 cost: 50.768060526332356\n",
      "j / n_batches: 282 / 410.0 cost: 50.70691364257907\n",
      "j / n_batches: 283 / 410.0 cost: 50.80412326302381\n",
      "j / n_batches: 284 / 410.0 cost: 50.63010140776181\n",
      "j / n_batches: 285 / 410.0 cost: 50.54502635953463\n",
      "j / n_batches: 286 / 410.0 cost: 50.60696155714939\n",
      "j / n_batches: 287 / 410.0 cost: 50.72731133484913\n",
      "j / n_batches: 288 / 410.0 cost: 50.72132109519392\n",
      "j / n_batches: 289 / 410.0 cost: 50.43846185429943\n",
      "j / n_batches: 290 / 410.0 cost: 50.36532361843853\n",
      "j / n_batches: 291 / 410.0 cost: 50.37265827624017\n",
      "j / n_batches: 292 / 410.0 cost: 50.39309049451842\n",
      "j / n_batches: 293 / 410.0 cost: 50.46842755134993\n",
      "j / n_batches: 294 / 410.0 cost: 50.40312831428866\n",
      "j / n_batches: 295 / 410.0 cost: 50.32376812450454\n",
      "j / n_batches: 296 / 410.0 cost: 50.24651618956513\n",
      "j / n_batches: 297 / 410.0 cost: 50.19095887126709\n",
      "j / n_batches: 298 / 410.0 cost: 50.14971268225381\n",
      "j / n_batches: 299 / 410.0 cost: 50.396053607967836\n",
      "j / n_batches: 300 / 410.0 cost: 50.03621053289991\n",
      "j / n_batches: 301 / 410.0 cost: 50.06503242488024\n",
      "j / n_batches: 302 / 410.0 cost: 49.97956976421061\n",
      "j / n_batches: 303 / 410.0 cost: 49.98046664867321\n",
      "j / n_batches: 304 / 410.0 cost: 49.76224029541848\n",
      "j / n_batches: 305 / 410.0 cost: 49.796135788478345\n",
      "j / n_batches: 306 / 410.0 cost: 49.94377932685702\n",
      "j / n_batches: 307 / 410.0 cost: 49.75517002409964\n",
      "j / n_batches: 308 / 410.0 cost: 49.71722938560757\n",
      "j / n_batches: 309 / 410.0 cost: 49.55812959303953\n",
      "j / n_batches: 310 / 410.0 cost: 49.8190385116795\n",
      "j / n_batches: 311 / 410.0 cost: 49.73654572124171\n",
      "j / n_batches: 312 / 410.0 cost: 49.56610969821444\n",
      "j / n_batches: 313 / 410.0 cost: 49.56031301799833\n",
      "j / n_batches: 314 / 410.0 cost: 49.6088500013884\n",
      "j / n_batches: 315 / 410.0 cost: 49.58400471752708\n",
      "j / n_batches: 316 / 410.0 cost: 49.27239496737656\n",
      "j / n_batches: 317 / 410.0 cost: 49.476955029826684\n",
      "j / n_batches: 318 / 410.0 cost: 49.31032043943133\n",
      "j / n_batches: 319 / 410.0 cost: 49.27035640379168\n",
      "j / n_batches: 320 / 410.0 cost: 49.35556034610288\n",
      "j / n_batches: 321 / 410.0 cost: 49.235656651346204\n",
      "j / n_batches: 322 / 410.0 cost: 49.35011980092361\n",
      "j / n_batches: 323 / 410.0 cost: 49.08581674154562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j / n_batches: 324 / 410.0 cost: 49.058878983291045\n",
      "j / n_batches: 325 / 410.0 cost: 48.95649441385096\n",
      "j / n_batches: 326 / 410.0 cost: 49.07293571564556\n",
      "j / n_batches: 327 / 410.0 cost: 48.954846431369475\n",
      "j / n_batches: 328 / 410.0 cost: 49.094170496547726\n",
      "j / n_batches: 329 / 410.0 cost: 48.93395157567474\n",
      "j / n_batches: 330 / 410.0 cost: 48.93236347209904\n",
      "j / n_batches: 331 / 410.0 cost: 48.96560053306892\n",
      "j / n_batches: 332 / 410.0 cost: 48.83898159110868\n",
      "j / n_batches: 333 / 410.0 cost: 48.87194193452536\n",
      "j / n_batches: 334 / 410.0 cost: 48.71610670115403\n",
      "j / n_batches: 335 / 410.0 cost: 48.718550777769586\n",
      "j / n_batches: 336 / 410.0 cost: 48.68512216046242\n",
      "j / n_batches: 337 / 410.0 cost: 48.612533329026725\n",
      "j / n_batches: 338 / 410.0 cost: 48.717504906218096\n",
      "j / n_batches: 339 / 410.0 cost: 48.671644074534036\n",
      "j / n_batches: 340 / 410.0 cost: 48.64913318784037\n",
      "j / n_batches: 341 / 410.0 cost: 48.37266792546684\n",
      "j / n_batches: 342 / 410.0 cost: 48.554328622722714\n",
      "j / n_batches: 343 / 410.0 cost: 48.5274316967094\n",
      "j / n_batches: 344 / 410.0 cost: 48.257091972224764\n",
      "j / n_batches: 345 / 410.0 cost: 48.446758350819366\n",
      "j / n_batches: 346 / 410.0 cost: 48.34831530724293\n",
      "j / n_batches: 347 / 410.0 cost: 48.22762589638171\n",
      "j / n_batches: 348 / 410.0 cost: 48.32231358754944\n",
      "j / n_batches: 349 / 410.0 cost: 48.21987234249249\n",
      "j / n_batches: 350 / 410.0 cost: 48.21856585952094\n",
      "j / n_batches: 351 / 410.0 cost: 48.138778765299016\n",
      "j / n_batches: 352 / 410.0 cost: 47.96872567248692\n",
      "j / n_batches: 353 / 410.0 cost: 48.1562961859051\n",
      "j / n_batches: 354 / 410.0 cost: 48.03106807347\n",
      "j / n_batches: 355 / 410.0 cost: 48.01342470724397\n",
      "j / n_batches: 356 / 410.0 cost: 48.04965579592083\n",
      "j / n_batches: 357 / 410.0 cost: 48.097759683810644\n",
      "j / n_batches: 358 / 410.0 cost: 48.18873820272363\n",
      "j / n_batches: 359 / 410.0 cost: 48.2411126636832\n",
      "j / n_batches: 360 / 410.0 cost: 47.933619439579516\n",
      "j / n_batches: 361 / 410.0 cost: 47.83689573075391\n",
      "j / n_batches: 362 / 410.0 cost: 47.81176115928632\n",
      "j / n_batches: 363 / 410.0 cost: 47.881720016971144\n",
      "j / n_batches: 364 / 410.0 cost: 47.9172713058981\n",
      "j / n_batches: 365 / 410.0 cost: 47.84352180686191\n",
      "j / n_batches: 366 / 410.0 cost: 47.72626249516508\n",
      "j / n_batches: 367 / 410.0 cost: 47.719693244330124\n",
      "j / n_batches: 368 / 410.0 cost: 47.524562284680144\n",
      "j / n_batches: 369 / 410.0 cost: 47.61255359048482\n",
      "j / n_batches: 370 / 410.0 cost: 47.54708882913323\n",
      "j / n_batches: 371 / 410.0 cost: 47.49876779600961\n",
      "j / n_batches: 372 / 410.0 cost: 47.59356307696017\n",
      "j / n_batches: 373 / 410.0 cost: 47.513294678482396\n",
      "j / n_batches: 374 / 410.0 cost: 47.464106175807316\n",
      "j / n_batches: 375 / 410.0 cost: 47.45604277158653\n",
      "j / n_batches: 376 / 410.0 cost: 47.56071923303725\n",
      "j / n_batches: 377 / 410.0 cost: 47.41575172722621\n",
      "j / n_batches: 378 / 410.0 cost: 47.39766448707246\n",
      "j / n_batches: 379 / 410.0 cost: 47.2901672191055\n",
      "j / n_batches: 380 / 410.0 cost: 47.292030137127334\n",
      "j / n_batches: 381 / 410.0 cost: 47.18447330784046\n",
      "j / n_batches: 382 / 410.0 cost: 47.277544701644885\n",
      "j / n_batches: 383 / 410.0 cost: 47.307395024593454\n",
      "j / n_batches: 384 / 410.0 cost: 47.22363646400183\n",
      "j / n_batches: 385 / 410.0 cost: 47.095948737373696\n",
      "j / n_batches: 386 / 410.0 cost: 47.26151341638922\n",
      "j / n_batches: 387 / 410.0 cost: 47.18588497425828\n",
      "j / n_batches: 388 / 410.0 cost: 47.139716208591665\n",
      "j / n_batches: 389 / 410.0 cost: 47.2957177365111\n",
      "j / n_batches: 390 / 410.0 cost: 47.124187513832524\n",
      "j / n_batches: 391 / 410.0 cost: 47.03306240543175\n",
      "j / n_batches: 392 / 410.0 cost: 46.90538872244514\n",
      "j / n_batches: 393 / 410.0 cost: 47.05993043179883\n",
      "j / n_batches: 394 / 410.0 cost: 46.924255970529984\n",
      "j / n_batches: 395 / 410.0 cost: 47.01565387282243\n",
      "j / n_batches: 396 / 410.0 cost: 46.899130644736154\n",
      "j / n_batches: 397 / 410.0 cost: 46.90656480605579\n",
      "j / n_batches: 398 / 410.0 cost: 46.9529389546979\n",
      "j / n_batches: 399 / 410.0 cost: 46.81654791782115\n",
      "j / n_batches: 400 / 410.0 cost: 46.72651371614938\n",
      "j / n_batches: 401 / 410.0 cost: 46.7581294007604\n",
      "j / n_batches: 402 / 410.0 cost: 46.86960804929649\n",
      "j / n_batches: 403 / 410.0 cost: 46.820636592277665\n",
      "j / n_batches: 404 / 410.0 cost: 46.78547556536643\n",
      "j / n_batches: 405 / 410.0 cost: 46.70103588623543\n",
      "j / n_batches: 406 / 410.0 cost: 46.61354097398657\n",
      "j / n_batches: 407 / 410.0 cost: 46.63190471206064\n",
      "j / n_batches: 408 / 410.0 cost: 46.424686523379975\n",
      "j / n_batches: 409 / 410.0 cost: 46.58768137917598\n",
      "training rbm: 1\n",
      "epoch: 0\n",
      "j / n_batches: 0 / 410.0 cost: 210.65342434630273\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest = getKaggleMNIST()\n",
    "dnn = DNN([1000, 750, 500], UnsupervisedModel=RBM)\n",
    "dnn.fit(Xtrain, Ytrain, Xtest, Ytest, epochs=3)\n",
    "\n",
    "# we compare with no pretraining in autoencoder.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM With TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the appropriate modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from util import getKaggleMNIST\n",
    "from autoencoder_tf import DNNclass RBM(object):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    def __init__(self, D, M, an_id):\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        self.id = an_id\n",
    "        self.build(D, M)\n",
    "\n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "\n",
    "    def build(self, D, M):\n",
    "        # params\n",
    "        self.W = tf.Variable(tf.random_normal(shape=(D, M)) * np.sqrt(2.0 / M))\n",
    "        # note: without limiting variance, you get numerical stability issues\n",
    "        self.c = tf.Variable(np.zeros(M).astype(np.float32))\n",
    "        self.b = tf.Variable(np.zeros(D).astype(np.float32))\n",
    "\n",
    "        # data\n",
    "        self.X_in = tf.placeholder(tf.float32, shape=(None, D))\n",
    "\n",
    "        # conditional probabilities\n",
    "        # NOTE: tf.contrib.distributions.Bernoulli API has changed in Tensorflow v1.2\n",
    "        V = self.X_in\n",
    "        p_h_given_v = tf.nn.sigmoid(tf.matmul(V, self.W) + self.c)\n",
    "        self.p_h_given_v = p_h_given_v # save for later\n",
    "        # self.rng_h_given_v = tf.contrib.distributions.Bernoulli(\n",
    "        #     probs=p_h_given_v,\n",
    "        #     dtype=tf.float32\n",
    "        # )\n",
    "        r = tf.random_uniform(shape=tf.shape(p_h_given_v))\n",
    "        H = tf.to_float(r < p_h_given_v)\n",
    "\n",
    "        p_v_given_h = tf.nn.sigmoid(\n",
    "            tf.matmul(H, tf.transpose(self.W)) + self.b)\n",
    "        # self.rng_v_given_h = tf.contrib.distributions.Bernoulli(\n",
    "        #     probs=p_v_given_h,\n",
    "        #     dtype=tf.float32\n",
    "        # )\n",
    "        r = tf.random_uniform(shape=tf.shape(p_v_given_h))\n",
    "        X_sample = tf.to_float(r < p_v_given_h)\n",
    "\n",
    "\n",
    "        # build the objective\n",
    "        objective = tf.reduce_mean(\n",
    "            self.free_energy(self.X_in)) - tf.reduce_mean(self.free_energy(X_sample))\n",
    "        self.train_op = tf.train.AdamOptimizer(1e-2).minimize(objective)\n",
    "        # self.train_op = tf.train.GradientDescentOptimizer(1e-3).minimize(objective)\n",
    "\n",
    "        # build the cost\n",
    "        # we won't use this to optimize the model parameters\n",
    "        # just to observe what happens during training\n",
    "        logits = self.forward_logits(self.X_in)\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=self.X_in,\n",
    "                logits=logits,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def fit(self, X, epochs=1, batch_sz=100, show_fig=False):\n",
    "        N, D = X.shape\n",
    "        n_batches = N // batch_sz\n",
    "\n",
    "        costs = []\n",
    "        print(\"training rbm: %s\" % self.id)\n",
    "        for i in range(epochs):\n",
    "            print(\"epoch:\", i)\n",
    "            X = shuffle(X)\n",
    "            for j in range(n_batches):\n",
    "                batch = X[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                _, c = self.session.run(\n",
    "                    (self.train_op, self.cost), feed_dict={self.X_in: batch})\n",
    "                if j % 10 == 0:\n",
    "                    print(\"j / n_batches:\", j, \"/\", n_batches, \"cost:\", c)\n",
    "                costs.append(c)\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "    def free_energy(self, V):\n",
    "        b = tf.reshape(self.b, (self.D, 1))\n",
    "        first_term = -tf.matmul(V, b)\n",
    "        first_term = tf.reshape(first_term, (-1,))\n",
    "\n",
    "        second_term = -tf.reduce_sum(\n",
    "            # tf.log(1 + tf.exp(tf.matmul(V, self.W) + self.c)),\n",
    "            tf.nn.softplus(tf.matmul(V, self.W) + self.c),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        return first_term + second_term\n",
    "\n",
    "    def forward_hidden(self, X):\n",
    "        return tf.nn.sigmoid(tf.matmul(X, self.W) + self.c)\n",
    "\n",
    "    def forward_logits(self, X):\n",
    "        Z = self.forward_hidden(X)\n",
    "        return tf.matmul(Z, tf.transpose(self.W)) + self.b\n",
    "\n",
    "    def forward_output(self, X):\n",
    "        return tf.nn.sigmoid(self.forward_logits(X))\n",
    "\n",
    "    def transform(self, X):\n",
    "        # accepts and returns a real numpy array\n",
    "        # unlike forward_hidden and forward_output\n",
    "        # which deal with tensorflow variables\n",
    "        return self.session.run(self.p_h_given_v, feed_dict={self.X_in: X})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest = getKaggleMNIST()\n",
    "\n",
    "# same as autoencoder_tf.py\n",
    "Xtrain = Xtrain.astype(np.float32)\n",
    "Xtest = Xtest.astype(np.float32)\n",
    "_, D = Xtrain.shape\n",
    "K = len(set(Ytrain))\n",
    "dnn = DNN(D, [1000, 750, 500], K, UnsupervisedModel=RBM)\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init_op)\n",
    "    dnn.set_session(session)\n",
    "    dnn.fit(Xtrain, Ytrain, Xtest, Ytest, pretrain=True, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
